#!/usr/bin/python3

import os
import os.path
import sys
import glob
import datetime
import fnmatch
os.chdir(os.path.join(os.path.dirname(os.path.realpath(__file__))))
sys.path.append("../modules")

from metro_support import lockFile, countFile
from db import *

# these variables are used for building:

builds = ( )
arches = ( )
subarches = ( )

# these variables, if defined, are used for repo management (cleaning, etc.):

all_builds = ( )
all_arches = ( )
all_subarches = ( )

cfgfile = os.path.join(os.path.expanduser("~"),".buildbot")
if os.path.exists(cfgfile):
    exec(open(cfgfile, "rb").read())
else:
    print("""
Create a ~/.buildbot file that contains something like this (python syntax):

builds = (
	"funtoo-experimental",
	"funtoo-current",
	"funtoo-current-hardened",
	"funtoo-stable",
)

arches = (
	"x86-32bit",
	"x86-64bit",
	"sparc-64bit",
	"pure64"
)

subarches = ( 
	"atom_32",
	"atom_64",
	"corei7",
	"corei7-pure64",
	"generic_32", 
	"i686", 
	"athlon-xp",
	"pentium4",
	"core2_32",
	"amd64-k8_32",
	"amd64-k8",
	"amd64-k10",
	"core2_64",
	"generic_64",
	"generic_64-pure64",
)

def map_build(build, subarch, full, full_date):
	# arguments refer to last build...
	if full == True:
		buildtype = "freshen"
	else:
		buildtype = "full"
	if subarch in [ "corei7", "corei7-pure64", "generic_64",  "generic_64-pure64" ]:
		buildtype = buildtype + "+openvz"
	return buildtype
""")
    sys.exit(1)

if len(all_builds) == 0:
	# if the all_variables are not defined, inherit values from the non-all vars:
	all_builds = builds
	all_arches = arches
	all_subarches = subarches

class SubArch(dbobject):
	@classmethod
	def _makeTable(cls,db):
		cls.db = db
		cls.__table__ = Table('subarch', db.metadata,
			Column('id', Integer, primary_key = True),
			Column('date', DateTime, index=True),
			Column('date_str', String, index=True),
			Column('path', String, index=True),
			Column('build', String, index=True),
			Column('arch', String, index=True),
			Column('subarch', String, index=True),
			Column('failcount', Integer, index=True),
			Column('full_date', DateTime, index=True),
			Column('full_date_str', String, index=True),
			Column('do_build', Boolean, index=True)
		)

class BuildDir(dbobject):
	@classmethod
	def _makeTable(cls,db):
		cls.db = db
		cls.__table__ = Table('bdir', db.metadata,
			Column('id', Integer, primary_key = True),
			Column('date', DateTime, index=True),
			Column('path', String, index=True),
			Column('build', String, index=True),
			Column('arch', String, index=True),
			Column('subarch', String, index=True),
			Column('date_str', String, index=True),
			Column('complete', Boolean, index=True),
			Column('full', Boolean, index=True)
		)

class Snapshot(dbobject):
	@classmethod
	def _makeTable(cls,db):
		cls.db = db
		cls.__table__ = Table('snapshot', db.metadata,
			Column('id', Integer, primary_key = True),
			Column('path', String, index=True),
			Column('build', String, index=True),
		)

class RepositoryDatabase(Database):
	__database__ = "sqlite:///cleaner.db"
	def __init__(self):
		Database.__init__(self,[BuildDir, Snapshot, SubArch])
		self.associate()
	def associate(self):
		Database.associate(self,self.__database__)

from subprocess import Popen, PIPE, call

process = Popen(["../metro", "-k", "path/mirror"], stdout=PIPE, stderr=PIPE)
out = process.stdout.read()
ret = process.wait()
if ret:
	print("Error calling metro for mirror path.")
else:
	initial_path = out.decode("ascii").strip()

if __name__ == "__main__":
	if os.path.exists("cleaner.db"):
		os.unlink("cleaner.db")
	db = RepositoryDatabase()
	session = db.session
	for build in all_builds:
		if not os.path.exists("%s/%s" % (initial_path, build)):
			continue
		snapdir = "%s/%s/snapshots" % ( initial_path, build )
		if os.path.isdir(snapdir) and not os.path.islink(snapdir):
			for match in glob.glob("%s/portage-*.tar.xz" % snapdir):
				basename = os.path.basename(match)
				if basename == "portage-current.tar.xz":
					continue
				sna = Snapshot()
				sna.path = match
				sna.build = build
				session.add(sna)
		for arch in all_arches:
			if not os.path.exists("%s/%s/%s" % ( initial_path, build, arch )):
				continue
			for subarch in all_subarches:
				path = "%s/%s/%s/%s" % (initial_path, build, arch, subarch)
				if not os.path.exists(path):
					continue
				most_recent = None
				most_recent_str = None
				most_recent_full = None
				most_recent_full_str = None
				failpath = path + "/.control/.failcount"
				if not os.path.exists(failpath):
					failcount = 0
				else:
					fc = countFile(failpath)
					failcount = fc.count
				for instance in os.listdir(path):
					try:
						date = datetime.datetime.strptime(instance,"%Y-%m-%d")
					except ValueError:
						continue
					ipath = "%s/%s" % ( path, instance )
					if not os.path.isdir(ipath):
						continue
					complete = False
					for match in glob.glob("%s/stage3*.tar.*" % ipath):
						complete = True
						break
					bdir = BuildDir()
					bdir.path = ipath
					bdir.date = date
					bdir.date_str = instance
					bdir.build = build
					bdir.arch = arch
					bdir.subarch = subarch
					bdir.complete = complete
					if complete:
						for match in glob.glob("%s/stage1*.tar.*" % ipath):
							bdir.full = True
							break
					session.add(bdir)
					if complete and ( most_recent == None or most_recent < bdir.date ):
						most_recent = bdir.date
						most_recent_str = bdir.date_str
						if bdir.full:
							most_recent_full = bdir.date
							most_recent_full_str = bdir.date_str
				sa = SubArch()
				sa.build = build
				sa.arch = arch
				sa.subarch = subarch
				sa.date = most_recent
				sa.date_str = most_recent_str
				sa.full_date = most_recent_full
				sa.full_date_str = most_recent_full_str
				sa.failcount = failcount
				sa.path = path
				# is subarch in our list of things to build, or just to maintain (ie. clean, for main repo)?
				if build in builds and arch in arches and subarch in subarches:
					sa.do_build = True
				else:
					sa.do_build = False
				session.add(sa)

	session.commit()
now = datetime.datetime.now()
stale_days = 2
# more than 3 fails and we remove the build from our rotation:
max_failcount = 3
def find_build(q):
	for x in q:
		# if something is newer than 4 days old, it is not considered stale, so we skip over it:
		if x.date != None and now - x.date < datetime.timedelta(days=stale_days):
			continue	
		# skip it if it is currently being built....
		tsfilepath = x.path+"/.control/.multi_progress"
		if os.path.exists(tsfilepath):
			tsfile = lockFile(tsfilepath)
			# ensure lockFile is not stale. The exists() method will clean it up automatically if it is:
			if tsfile.exists():
				sys.stderr.write("# Build at %s in progress, skipping...\n" % x.path)
				continue
		# output: build subarch was-last-build-full(had-a-stage-1)(boolean) date
		print("build=%s" % x.build)
		print("arch_desc=%s" % x.arch)
		print("subarch=%s" % x.subarch)
		print("fulldate=%s" % x.full_date_str)
		print("nextdate=%s" % datetime.datetime.strftime(datetime.datetime.now(), "%Y-%m-%d"))
		print("failcount=%s" % x.failcount)
		mb = map_build(x.build, x.subarch, x.full_date == x.date and x.date != None, x.full_date_str)
		# handle case where map_build returns "full+openvz":
		if type(mb) == str:
			mb = mb.split("+")
		# handle case where map_build returns ("full", "openvz"):
		print("target=%s" % mb[0])
		if len(mb) > 1:
			print("extras='%s'" % " ".join(mb[1:]))
		else:
			print("extras=''")
		sys.exit(0)

if len(sys.argv) > 1 and sys.argv[1] == "clean":
	for build in all_builds:
		# first -- remove any more than 3 complete builds, starting with oldest, of course:
		for arch in all_arches:
			for subarch in all_subarches:
				out = session.query(BuildDir).filter_by(build=build).filter_by(arch=arch).filter_by(subarch=subarch).filter_by(complete=True).order_by(BuildDir.date_str).all()
				for x in out[0:-3]:
					print(("rm -rf %s" % x.path))
				for x in out[-3:]:
					print(("# keeping %s" % x.path))
		# next, remove any more than 2 snapshots:
		sna = session.query(Snapshot).filter_by(build=build).order_by(Snapshot.path).all()
		for x in sna[0:-3]:
			print(("rm %s*" % x.path))
		for x in sna[-2:]:
			print(("# keeping %s" % x.path))
	# Now, look at incomplete builds. Clean them out when they're stale and not the most recent build.
	for x in session.query(BuildDir).filter_by(complete=False):
		# ignore non-stale builds for cleaning -- they could be in-progress...
		y = session.query(SubArch).filter_by(subarch=x.subarch).filter_by(arch=x.arch).filter_by(build=x.build).first()
		if x.date != None and now - x.date > datetime.timedelta(days=stale_days) and y.date != x.date:
			# don't zap most recent...
			print(("rm -rf %s* # not complete and stale, not most recent build" % x.path))
		else:
			print("# keeping %s" % x.path)

elif len(sys.argv) > 1 and sys.argv[1] == "nextbuild":
	sa = session.query(SubArch)
	myfail = 0
	while myfail < max_failcount:
		# we start with failcount of zero, and prioritize not-yet-built stages:
		empties = sa.filter(SubArch.__table__.c.do_build == True).filter(SubArch.__table__.c.date == None).filter(SubArch.__table__.c.build.in_(builds)).filter_by(failcount=myfail)
		find_build(empties)
		# if we can't find one, we look for a failcount of zero, but prioritize by date:
		oldies = sa.filter(SubArch.__table__.c.do_build == True).filter(SubArch.__table__.c.date != None).filter(SubArch.__table__.c.build.in_(builds)).filter_by(failcount=myfail).order_by(SubArch.__table__.c.date)
		find_build(oldies)
		myfail += 1
		# if no builds are found, we increment failcount, and try again:
	# THIS PART IS DISABLED. IF A BUILD IS BEYOND ITS FAILCOUNT, IT IS OUT OF ROTATION UNTIL SOMEONE MANUALLY INTERVENES:
	# if our loop didn't present a build, we ignore date, look for all builds with failcount >= our max, and then order by failcount, and pick a build:
	#baddies = sa.filter(SubArch.__table__.c.do_build == True).filter(SubArch.__table__.c.build.in_(builds)).filter(SubArch.__table__.c.failcount >= failcount).order_by(SubArch.__table__.c.failcount.asc())
	#find_build(baddies)
	sys.exit(1)
elif len(sys.argv) ==2 and sys.argv[1] == "empties":
	empties = session.query(SubArch).filter(SubArch.__table__.c.do_build == True).filter(SubArch.__table__.c.date == None).filter(SubArch.__table__.c.build.in_(builds))
	for x in empties:
		print(x.path)
elif len(sys.argv) == 2 and sys.argv[1] == "fails":
	fails = session.query(SubArch).filter(SubArch.__table__.c.failcount > 0).order_by(SubArch.__table__.c.failcount.desc())
	goods = session.query(SubArch).filter(SubArch.__table__.c.failcount == 0)
	for x in fails:
		print(str(x.failcount).rjust(4), "None".rjust(12) if not x.date else datetime.datetime.strftime(x.date, "%Y-%m-%d").rjust(12), x.path)
	for x in goods:
		print(str(x.failcount).rjust(4), "None".rjust(12) if not x.date else datetime.datetime.strftime(x.date, "%Y-%m-%d").rjust(12), x.path)
elif len(sys.argv) == 2 and sys.argv[1] == "index.xml":
	from lxml import etree
	from copy import deepcopy
	"""
	<subarches>
		<subarch name="amd64-bulldozer" builds="1,2">
			<build id="1" variant="pure64,hardened" build="funtoo-current" path="/funtoo-current/blah/blah" latest="2014-12-31"/>
			<build id="2" variant="" build="funtoo-current" path="/funtoo-current/blah/blah" latest="2014-12-31"/>
		</subarch>
		<fails>
			<build id="1" variant="pure64,hardened" build="funtoo-current" path="/funtoo-current/blah/blah" latest="2014-12-31" failcount="2"/>
		</fails>
	</subarches>
"""
	outxml = etree.Element("subarches")
	fails = etree.Element("fails")
	outxml.append(fails)
	subarch_builds = {}
	for x in session.query(SubArch):
		s = x.subarch
		if s[-7:] == "-pure64":
			s = s[:-7]
		if s not in subarch_builds:
			subarch_builds[s] = []
		subarch_builds[s].append(x)
	numfails = 0
	for s in subarch_builds:
		subarch_xml = etree.Element("subarch")
		subarch_xml.attrib["name"] = s
		subarch_xml.attrib["builds"] = ",".join(map(str,range(1,len(subarch_builds[s])+1)))
		# we treat "hardened' and "pure64" as "variants" of the main build (current/stable/experimental) -- to make things clearer for users.
		count = 1
		for b in subarch_builds[s]:
			# variant
			v = []
			if b.subarch[-7:] == "-pure64":
				# "variant"
				v.append("pure64")
			if b.build[-9:] == "-hardened":
				v.append("hardened")
				build = b.build[:-9]
			else:
				build = b.build
			build_xml = etree.Element("build")
			build_xml.attrib["id"] = str(count)
			build_xml.attrib["variant"] = ','.join(v)
			build_xml.attrib["build"] = build
			build_xml.attrib["path"] = b.path[len(initial_path):]
			build_xml.attrib["latest"] = "None" if not b.date else datetime.datetime.strftime(b.date, "%Y-%m-%d") 
			if b.date:
				build_xml.attrib["download"] = b.path[len(initial_path):] + "/stage3-" + b.subarch + "-" + b.build + "-" + build_xml.attrib["latest"] + ".tar.xz"
			subarch_xml.append(build_xml)
			if b.failcount > 0:
				numfails += 1
				fail_xml = deepcopy(build_xml)
				fail_xml.attrib["id"] = str(numfails)
				fail_xml.attrib["failcount"] = str(b.failcount)
				fails.append(fail_xml)
			count += 1
		outxml.append(subarch_xml)
	fails.attrib["builds"] = ",".join(map(str,range(1,numfails + 1)))
 
	outf = open(os.path.join(initial_path,"index.xml"),"wb")
	outf.write(etree.tostring(outxml, encoding="UTF-8", pretty_print=True, xml_declaration=True))

elif len(sys.argv) == 2 and sys.argv[1] == "zap":
	for x in session.query(SubArch).filter(SubArch.__table__.c.do_build == True).filter(SubArch.__table__.c.failcount > 0):
		failpath = "%s/.control/.failcount" % x.path
		if os.path.exists(failpath):
			print("Removing %s..." % failpath)
			os.unlink(failpath)
elif len(sys.argv) > 1 and sys.argv[1] == "digestgen":
	print("Generating hashes...")
	links = []
	for root, dirnames, filenames in os.walk(initial_path):
		for filename in fnmatch.filter(filenames, '*.tar.*'):
			if filename[-4:] == ".txt":
				continue
			p = os.path.join(root, filename)
			if os.path.islink(p):
				links.append(os.path.join(dirnames,p))
				continue
			if not os.path.exists(p+".hash.txt"):
				sys.stdout.write('.')
				sys.stdout.flush()
				call("PATH=/bin/:/usr/bin echo sha256 $(sha256sum %s | cut -f1 -d' ') > %s.hash.txt" % ( p, p ), shell=True)
			else:
				sys.stdout.write('e')
				sys.stdout.flush()
	for link in links:
		abs_realpath = os.path.normpath(os.path.join(os.path.dirname(link),os.readlink(link)))
		if os.path.lexists(link + ".hash.txt"):
			os.unlink(link + ".hash.txt")
		if os.path.exists(abs_realpath + ".hash.txt"):
			os.symlink(os.readlink(link) + ".hash.txt", link + ".hash.txt")
		else:
			print((link + " dead; cleaning up"))
			os.unlink(link)

	print()
	print("Cleaning up old hashes...")
	for root, dirnames, filenames in os.walk(initial_path):
		for filename in fnmatch.filter(filenames, '*.hash.txt'):
			p = os.path.join(root, filename)
			ptar = p[:-9]
			if not os.path.exists(ptar):
				os.unlink(p)
				sys.stdout.write('.')
				sys.stdout.flush()

	print()
	print("Done!")
